\chapter{Conclusion et Perspectives}

\section{Bilan du Projet}

Le projet SearchBook a permis de développer une solution complète et fonctionnelle de moteur de recherche pour bibliothèque numérique. En partant d'un cahier des charges ambitieux imposant un volume conséquent de données (1664 livres) et des fonctionnalités avancées, nous avons réussi à concevoir une architecture logicielle capable de relever ces défis.

Les objectifs principaux ont été atteints :
\begin{itemize}
    \item \textbf{Ingestion Robuste} : Le pipeline de données permet de constituer et de maintenir une bibliothèque de plusieurs milliers d'ouvrages.
    \item \textbf{Recherche Performante} : L'indexation inversée garantit des temps de réponse inférieurs à 50ms pour la majorité des requêtes.
    \item \textbf{Fonctionnalités Avancées} : L'intégration des Regex et des algorithmes de graphes (Jaccard, Closeness) enrichit considérablement l'expérience utilisateur par rapport à une simple recherche textuelle.
    \item \textbf{Interface Moderne} : L'application web offre une ergonomie fluide et intuitive.
\end{itemize}

Ce projet a également été l'occasion d'approfondir notre compréhension des problématiques liées au Big Data textuel (indexation, complexité algorithmique) et à l'architecture des systèmes distribués.

\section{Perspectives d'Amélioration}

Bien que fonctionnel, le système actuel possède des marges de progression intéressantes :

\subsection{Amélioration Sémantique}
L'utilisation actuelle de Jaccard et BM25 repose sur une approche purement lexicale. L'intégration de modèles de langage neuronaux (Transformers, BERT) permettrait de passer à une \textbf{recherche sémantique}. En vectorisant les livres (Embeddings) et en utilisant une base de données vectorielle (comme pgvector ou Milvus), nous pourrions suggérer des livres traitant des mêmes thèmes même s'ils n'utilisent pas les mêmes mots.

\subsection{Scalabilité Horizontale}
Pour gérer non plus 1600 mais 16 millions de livres, l'architecture monolithique de la base de données deviendrait un frein. La mise en place de \textbf{Sharding} (partitionnement des données sur plusieurs serveurs) et l'utilisation de moteurs d'indexation distribués comme Elasticsearch ou Solr seraient indispensables.

\subsection{Algorithmes de Graphe Approchés}
Le calcul exact de la centralité et des similarités est coûteux ($O(N^2)$). L'utilisation d'algorithmes probabilistes comme \textbf{MinHash} ou \textbf{HyperLogLog} permettrait d'estimer ces métriques avec une erreur minime mais pour un coût de calcul drastiquement réduit, rendant le système viable à très grande échelle.

En conclusion, SearchBook constitue une base solide et extensible pour la gestion de connaissances personnelles, démontrant la puissance de l'alliance entre algorithmique classique et technologies web modernes.
