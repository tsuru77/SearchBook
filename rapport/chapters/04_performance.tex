\chapter{Tests de Performance et Analyse}

La validation expérimentale est une étape cruciale pour s'assurer que notre système répond aux exigences de performance, notamment avec un corpus de 1664 livres. Ce chapitre décrit notre méthodologie de test et analyse les résultats obtenus.

\section{Protocole de Test}

\subsection{Environnement de Test}
Les tests ont été réalisés sur une machine standard (MacBook Pro M1, 16GB RAM) exécutant l'application via Docker. Cette configuration est représentative d'un environnement de développement ou d'un petit serveur de production.

\subsection{Constitution du Testbed (Corpus)}
Pour obtenir un corpus réaliste et conforme aux spécifications (1664 livres, >10k mots), nous avons utilisé notre script d'ingestion automatisé connecté à l'API du Projet Gutenberg.
\begin{itemize}
    \item \textbf{Source} : Project Gutenberg (miroir).
    \item \textbf{Filtrage} : Seuls les livres en anglais ayant plus de 10 000 mots ont été conservés.
    \item \textbf{Volume Final} : 1664 livres, représentant environ 150 millions de mots au total.
\end{itemize}

\section{Résultats et Analyse}

\subsection{Performance de l'Ingestion}
L'ingestion est une opération coûteuse mais unique (ou rare).
\begin{itemize}
    \item \textbf{Phase 1 (Chargement)} : Le téléchargement et l'insertion en base des 1664 livres ont pris environ \textbf{45 minutes}. Le goulot d'étranglement principal est la latence réseau vers Gutenberg et l'insertion ligne à ligne. L'utilisation de \texttt{COPY} en PostgreSQL a permis d'optimiser cette étape.
    \item \textbf{Phase 2 (Calcul de Graphe)} : Le calcul des similarités de Jaccard ($N^2$ comparaisons) a pris environ \textbf{12 minutes}. C'est un résultat très satisfaisant, rendu possible par l'utilisation de structures d'ensembles (Sets) en Python et le filtrage précoce des paires non pertinentes.
\end{itemize}

\subsection{Performance de la Recherche (Temps de Réponse)}
Nous avons mesuré le temps de réponse moyen pour 100 requêtes aléatoires de complexité variable.

\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|}
\hline
\textbf{Type de Requête} & \textbf{Temps Moyen (ms)} & \textbf{Complexité Observée} \\
\hline
Mot-clé unique (ex: "whale") & 15 ms & $O(1)$ \\
\hline
Multi-mots (ex: "white whale") & 45 ms & $O(k)$ \\
\hline
Regex Simple (ex: "wha.*le") & 120 ms & $O(n_{filtered})$ \\
\hline
Regex Complexe (ex: "(a|b)+c.*d") & 850 ms & $O(n_{full})$ \\
\hline
\end{tabular}
\caption{Temps de réponse moyen par type de recherche}
\end{table}

\textbf{Analyse} :
La recherche par mot-clé est extrêmement rapide (< 50ms), validant l'efficacité de l'index inversé B-Tree. L'expérience utilisateur est fluide, quasi-instantanée.
La recherche par Regex montre une dégradation logique des performances. Les regex simples bénéficient de l'index trigramme, restant sous la barre des 200ms. Les regex complexes nécessitant un scan complet peuvent prendre près d'une seconde, ce qui reste acceptable pour une fonctionnalité "avancée" sur un tel volume de données, mais montre les limites de l'approche sans indexation spécialisée pour automates.

\subsection{Scalabilité}
Nous avons observé l'évolution du temps de réponse en fonction de la taille du corpus (de 100 à 1664 livres).
La courbe de temps de réponse pour la recherche par mot-clé reste plate (logarithmique), ce qui est excellent.
La courbe pour la construction du graphe est quadratique ($O(N^2)$), ce qui confirme que cette étape ne passera pas à l'échelle pour des millions de livres sans changer d'algorithme (ex: utiliser MinHash LSH pour approximer Jaccard en temps sous-linéaire).

\section{Discussion}

Les résultats démontrent que l'architecture choisie est capable de gérer la charge demandée. Le système est robuste pour une bibliothèque personnelle de cette taille. Cependant, pour passer à l'échelle du web (millions de livres), des changements architecturaux majeurs seraient nécessaires : sharding de la base de données, utilisation d'un moteur de recherche dédié comme Elasticsearch au lieu de PostgreSQL pour le texte, et algorithmes d'approximation pour les graphes.
